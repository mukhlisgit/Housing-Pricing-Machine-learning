# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cFCa86tDdItvbGx2DGNBNM5R9b_yCKll
"""

import numpy as np # linear algebra
import pandas as pd

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

from sklearn import linear_model
from sklearn.linear_model import LinearRegression

train_data=pd.read_csv('/content/train.csv')
test_data=pd.read_csv('/content/test.csv')

#box plot overallqual/saleprice
var = 'OverallQual'
data = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)
f, ax = plt.subplots(figsize=(8, 6))
fig = sns.boxplot(x=var, y="SalePrice", data=data)
fig.axis(ymin=0, ymax=800000);

train_data.describe().T

corrmat = train_data.corr()
plt.subplots(figsize=(16,8))
sns.heatmap(corrmat,cmap="YlGnBu")

Corr_Column = list(train_data.corr()["SalePrice"][(train_data.corr()["SalePrice"]>0.50) | (train_data.corr()["SalePrice"]<-0.50)].index)

#print(Corr_Column)

sns.pairplot(train_data[Corr_Column])

sns.distplot(train_data['SalePrice'])
plt.show()

train_data.info()

#creating categorical features using train_data dataset
categorical_features=[feature for feature in train_data.columns if train_data[feature].dtypes=='O']
print('Number of categorical variables: ', len(categorical_features))

train_data[categorical_features].head()

#finding out all the missing values in each column in train_data
features_nan=[feature for feature in train_data.columns if train_data[feature].isnull().sum() and train_data[feature].dtypes=='O']

for feature in features_nan:
    print("{}: {}% missing values".format(feature,np.round(train_data[feature].isnull().mean(),3)))

# now i will replace each missing value with a new label 
def replace_cat_feature(dataset,features_nan):
    train_data[features_nan]=train_data[features_nan].fillna('Missing')
    return train_data

dataset=replace_cat_feature(train_data,features_nan)

# df_train[features_nan].isnull().sum()

# Now lets check for numerical variables the contains missing values
numerical_with_nan=[feature for feature in train_data.columns if train_data[feature].isnull().sum() and train_data[feature].dtypes!='O']

# We will print the numerical nan variables and percentage of missing values

for feature in numerical_with_nan:
    print("{}: {}% missing value".format(feature,np.around(train_data[feature].isnull().mean(),3)))

for feature in numerical_with_nan:
    # We will replace by using mean since there are outliers
    mean_value = train_data[feature].mean()
    
    # create a new feature to capture nan values
    train_data[feature+'nan']=np.where(train_data[feature].isnull(),1,0)
    train_data[feature].fillna(mean_value,inplace=True)

for feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:
       
    train_data[feature] = train_data['YrSold'] - train_data[feature]

train_data[['YearBuilt','YearRemodAdd','GarageYrBlt']].head()

categorical_features

for feature in categorical_features:
    temp=train_data.groupby(feature)['SalePrice'].count()/len(train_data)
    temp_df=temp[temp>0.01].index
    train_data[feature]=np.where(train_data[feature].isin(temp_df),train_data[feature],'Rare_var')

train_data.head()

# Plotting Categorical Features with Sale Price
def facetgrid_boxplot(x, y, **kwargs):
    sns.boxplot(x=x, y=y)
    x=plt.xticks(rotation=90)
    
categorical = train_data.select_dtypes(exclude=['int64','float64'])
f = pd.melt(train_data, id_vars=['SalePrice'], value_vars=sorted(train_data[categorical.columns]))
g = sns.FacetGrid(f, col="variable", col_wrap=3, sharex=False, sharey=False, size=5)
g = g.map(facetgrid_boxplot, "value", "SalePrice")

for feature in categorical_features:
    labels_ordered=train_data.groupby([feature])['SalePrice'].mean().sort_values().index
    labels_ordered={k:i for i,k in enumerate(labels_ordered,0)}
    train_data[feature]=train_data[feature].map(labels_ordered)

train_data.head(20)

# Handling outliers 
num_col = list(train_data.dtypes[train_data.dtypes !='object'].index)
num_col = ['LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','TotalBsmtSF','1stFlrSF','GrLivArea','OpenPorchSF',
           'EnclosedPorch','3SsnPorch','ScreenPorch' ,'PoolArea','MiscVal','SalePrice']
def drop_outliers(x):
    list = []
    for col in num_col:
        Q1 = x[col].quantile(.25)
        Q3 = x[col].quantile(.99)
        IQR = Q3-Q1
        x =  x[(x[col] >= (Q1-(1.5*IQR))) & (x[col] <= (Q3+(1.5*IQR)))] 
    return x   

train_data = drop_outliers(train_data)

from scipy import stats
from scipy.stats import norm

# Drawing the distribution plot with normal distribution fitted curve
# Drawing the Quantile-Quantile plot 
  
def normality_plot(X):

    fig, axes = plt.subplots(1, 2, figsize=(10, 5))

    sns.distplot(X, fit=norm, ax=axes[0])
    axes[0].set_title('Distribution Plot')

    axes[1] = stats.probplot((X), plot=plt)
    plt.tight_layout()

y = train_data.SalePrice
normality_plot(y)

# in order normalize skewed data we can use log transformation
#because big values will be pulled to the center. 

# but since log(0) = Nan, i used log(1+X) 

y = np.log(1 + y)
normality_plot(y)

feature_scale=[feature for feature in train_data.columns if feature not in ['SalePrice']]

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
scaler.fit(train_data[feature_scale])

scaler.transform(train_data[feature_scale])

train_data = pd.concat([train_data[['SalePrice']].reset_index(drop=True),
                    pd.DataFrame(scaler.transform(train_data[feature_scale]), columns=feature_scale)],
                    axis=1)

train_data.head()

# Distribution after minmaxscaling
plt.figure(figsize=(16,6))
plt.subplot(121)
sns.distplot(train_data.SalePrice)

## Missing values 

features_nan=[feature for feature in test_data.columns if test_data[feature].isnull().sum() and test_data[feature].dtypes=='O']

for feature in features_nan:
    print("{}: {}% missing values".format(feature,np.round(test_data[feature].isnull().mean(),3)))

# Replace missing value with a new label 
def replace_cat_feature(dataset,features_nan):
    test_data[features_nan]=test_data[features_nan].fillna('Missing')
    return train_data

dataset=replace_cat_feature(test_data,features_nan)

dataset[features_nan].isnull().sum()
test_data.head()

# Numerical variables 

# checking for numerical variables the contains missing values
numerical_with_nan=[feature for feature in test_data.columns if test_data[feature].isnull().sum() and test_data[feature].dtypes!='O']

# print the numerical nan variables and percentage of missing values

for feature in numerical_with_nan:
    print("{}: {}% missing value".format(feature,np.around(test_data[feature].isnull().mean(),3)))

#we will now replace each missing value
for feature in numerical_with_nan:
    #We will replace by using mean 
    mean_value = test_data[feature].mean()
    
    # create a new feature to capture nan values
    test_data[feature+'nan']=np.where(test_data[feature].isnull(),1,0)
    test_data[feature].fillna(mean_value,inplace=True)
    
test_data[numerical_with_nan].isnull().sum()
test_data.head()

# Categorical features --> Encoding 
from sklearn.preprocessing import LabelEncoder

cols = ('MSZoning','Street','Alley','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood','Condition1','Condition2',
        'BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','ExterQual','ExterCond','Foundation','BsmtQual',
        'BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','Heating','HeatingQC','CentralAir','Electrical','KitchenQual','Functional',
        'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive','PoolQC','Fence','MiscFeature','SaleType','SaleCondition')

# process columns, apply LabelEncoder to categorical features
for c in cols:
    lbl = LabelEncoder() 
    lbl.fit(list(test_data[c].values)) 
    test_data[c] = lbl.transform(list(test_data[c].values))

test_data.head()

from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score

# Load models
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.svm import SVR
from sklearn import tree
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

#building training and test data
data = train_data.select_dtypes(include=[np.number]).interpolate().dropna()
test = test_data.select_dtypes(include=[np.number]).interpolate().dropna()

X = data.drop(['SalePrice'], axis=1)
y = np.log(train_data.SalePrice)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

# Linear Regression
from sklearn import linear_model
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
# Create linear regression object
regr = linear_model.LinearRegression()

# Train the model using the training sets
regr.fit(X_train, y_train)

# Make predictions using the testing set
y_pred = regr.predict(X_test)

print('MSE is:', mean_squared_error(y_test, y_pred))
rmse_lin_reg = np.sqrt(mean_squared_error(y_test, y_pred))
print("RMSE for Linear Regression is:", rmse_lin_reg)
print('The accuracy of the Linear Regression is',r2_score(y_test,y_pred))

# Ridge 

ridge = Ridge()
param_grid = {'alpha': [0.001, 0.01, 0.03, 0.05, 0.09, 0.7, 0.9, 5, 10, 20, 100]}
grid_search_ridge = GridSearchCV(ridge, param_grid, cv = 5)
grid_search_ridge.fit(X_train, y_train)
y_pred = grid_search_ridge.predict(X_test)

print ('MSE is:', mean_squared_error(y_test, y_pred))

rmse_ridge_reg = np.sqrt(mean_squared_error(y_test, y_pred))
print("RMSE for Ridge Regression is:", rmse_ridge_reg)
print('The accuracy of the Ridge Regression is',r2_score(y_test,y_pred))

# SVR 

svr = SVR()
param_grid = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'gamma': ['scale', 'auto'],
              'C': [1, 5, 10], 'epsilon': [0.1, 1, 10]}
grid_search_svr = GridSearchCV(svr, param_grid, cv = 5)
grid_search_svr.fit(X_train, y_train)
y_pred = grid_search_svr.predict(X_test)

print('MSE is:', mean_squared_error(y_test, y_pred))

rmse_svr_reg = np.sqrt(mean_squared_error(y_test, y_pred))
print("RMSE of SVR is:", rmse_svr_reg)
print('The accuracy of the Support Vector Regression is',r2_score(y_test,y_pred))

# Decision Tree Regressor 

dtr = DecisionTreeRegressor(random_state = 0)
param_grid = {'max_depth': list(range(2, 10)),
              'splitter': ['best', 'random'],
              'min_samples_leaf': list(range(1, 10)),
              'max_leaf_nodes': list(range(5, 20))}
grid_search_dtr = GridSearchCV(dtr, param_grid, cv = 5)
grid_search_dtr.fit(X_train, y_train)
y_pred = grid_search_dtr.predict(X_test)


print('MSE is:', mean_squared_error(y_test, y_pred))
rmse_dtr_reg = np.sqrt(mean_squared_error(y_test, y_pred))
print("RMSE of DT is:", rmse_dtr_reg)
print('The accuracy of the Decision Tree Regressor is',r2_score(y_test,y_pred))
print(grid_search_dtr.best_params_)

# Random Forest Regressor 

rfr = RandomForestRegressor()
param_grid = {'n_estimators': list(range(100, 200, 10)),
             'max_depth': list(range(4, 7)),
             'min_samples_split': list(range(2, 4))}
grid_search_rfr = GridSearchCV(rfr, param_grid, cv = 5)
grid_search_rfr.fit(X_train, y_train)
y_pred = grid_search_rfr.predict(X_test)

print('MSE is:', mean_squared_error(y_test, y_pred))

rmse_rfr_reg = np.sqrt(mean_squared_error(y_test, y_pred))
print("RMSE of RF is:", rmse_rfr_reg)
print('The accuracy of Random Forest Regression is',r2_score(y_test,y_pred))

# Comparison of models 

data = {'Linear Regression': rmse_lin_reg, 'Ridge Regression': rmse_ridge_reg,
        'Support Vector Regressor': rmse_svr_reg, 'Decision Tree Regressor': rmse_dtr_reg, 'Random Forest Regressor': rmse_rfr_reg}
data = dict(sorted(data.items(), key = lambda x: x[1], reverse = True))
models = list(data.keys())
RMSE = list(data.values())
fig = plt.figure(figsize = (30, 10))
sns.barplot(x = models, y = RMSE)
plt.xlabel("Models Used", size = 20)
plt.xticks(rotation = 30, size = 15)
plt.ylabel("RMSE", size = 20)
plt.yticks(size = 15)
plt.title("RMSE for different models", size = 25)
plt.show()